{
  "name": "llamacpp-ai-provider",
  "version": "0.0.8",
  "description": "Vercel AI Provider for running LLMs locally using LLamaCpp",
  "main": "./dist/index.js",
  "scripts": {
    "setup": "./bin/update_submodules.sh",
    "build": "cmake-js -d ./llamacpp build && tsc",
    "clean": "cmake-js -d ./llamacpp rebuild && tsc --build --clean",
    "prepublish": "npm run clean",
    "pretest": "npm run build",
    "postversion": "git push && git push --tags",
    "test": "NODE_OPTIONS=--experimental-vm-modules jest ./dist/**/*.test.js"
  },
  "type": "module",
  "keywords": [
    "llm",
    "genai",
    "vercel-ai"
  ],
  "author": "Nick Nance",
  "license": "MIT",
  "devDependencies": {
    "@types/jest": "^29.5.12",
    "@types/node": "^20.12.2",
    "cmake-js": "^7.3.0",
    "jest": "^29.7.0",
    "prettier": "^3.2.5",
    "tsx": "^4.7.1",
    "typescript": "^5.4.3"
  },
  "peerDependencies": {
    "ai": "^3.0.16"
  },
  "files": [
    "dist",
    "llamacpp/build/Release"
  ],
  "dependencies": {
    "node-addon-api": "^8.0.0"
  }
}
